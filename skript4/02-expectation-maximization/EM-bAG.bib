
@book{koller_probabilistic_2009,
	series = {Adaptive Computation and Machine Learning},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {9780262013192},
	shorttitle = {Probabilistic Graphical Models},
	language = {en},
	urldate = {2014-10-04},
	publisher = {{MIT} Press},
	author = {Koller, Daphne and Friedman, Nir},
	year = {2009},
	file = {Koller and Friedman - 2009 - Probabilistic graphical models principles and tec.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/1fz1fwch.default/zotero/storage/TIQ6DUNM/Koller and Friedman - 2009 - Probabilistic graphical models principles and tec.pdf:application/pdf}
}

@book{murphy_machine_2012,
	title = {Machine learning: a probabilistic perspective},
	isbn = {9780262018029},
	shorttitle = {Machine Learning},
	url = {http://www.cs.ubc.ca/~murphyk/MLbook/},
	abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a {MATLAB} software package--{PMTK} (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
	language = {en},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P.},
	month = aug,
	year = {2012},
	file = {Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/1fz1fwch.default/zotero/storage/38NTNB8K/Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:application/pdf}
}

@book{bishop_pattern_2006,
	edition = {1},
	series = {Information science and statistics},
	title = {Pattern recognition and machine learning},
	isbn = {9780387310732},
	abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	month = aug,
	year = {2006},
	file = {Bishop - 2006 - Pattern recognition and machine learning.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/1fz1fwch.default/zotero/storage/C5AUW85K/Bishop - 2006 - Pattern recognition and machine learning.pdf:application/pdf}
}

@article{anderson_maximum-likelihood_1985,
	title = {Maximum-likelihood estimation of the parameters of a multivariate normal distribution},
	volume = {70},
	issn = {0024-3795},
	url = {http://www.sciencedirect.com/science/article/pii/0024379585900497},
	doi = {10.1016/0024-3795(85)90049-7},
	abstract = {This paper provides an exposition of alternative approaches for obtaining maximum- likelihood estimators ({MLE}) for the parameters of a multivariate normal distribution under different assumptions about the parameters. A central focus is on two general techniques, namely, matrix differentiation and matrix transformations. These are systematically applied to derive the {MLE} of the means under a rank constraint and of the covariances when there are missing observations. Derivations using induction and inequalities are also included to illustrate alternative methods. Other examples, such as a connection with an econometric model, are included. Although the paper is primarily expository, some of the proofs are new.},
	urldate = {2014-10-07},
	journal = {Linear Algebra and its Applications},
	author = {Anderson, T. W. and Olkin, I.},
	month = oct,
	year = {1985},
	keywords = {readme},
	pages = {147--171},
	file = {Anderson and Olkin - 1985 - Maximum-likelihood estimation of the parameters of.pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/1fz1fwch.default/zotero/storage/Z88THFZG/Anderson and Olkin - 1985 - Maximum-likelihood estimation of the parameters of.pdf:application/pdf}
}

@misc{kaggle.com_classify_2012,
	type = {Online competition},
	title = {Classify handwritten digits using the famous {MNIST} data},
	url = {https://www.kaggle.com/c/digit-recognizer},
	abstract = {This competition is the first in a series of tutorial competitions designed to introduce people to Machine Learning.

The goal in this competition is to take an image of a handwritten single digit, and determine what that digit is.  As the competition progresses, we will release tutorials which explain different machine learning algorithms and help you to get started.},
	language = {en},
	urldate = {2014-10-12},
	journal = {kaggle.com},
	author = {Kaggle.com},
	month = jul,
	year = {2012}
}

@misc{lecun_mnist_2012,
	title = {The {MNIST} database of handwritten digits},
	url = {http://yann.lecun.com/exdb/mnist/index.html},
	abstract = {The {MNIST} database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from {NIST}. The digits have been size-normalized and centered in a fixed-size image.},
	language = {en},
	urldate = {2014-10-10},
	journal = {Yan {LeCun}'s homepage},
	author = {LeCun, Yan and Cortes, Corinna and Burges, Christopher J.C.},
	year = {2012},
	file = {LeCun et al. - 2012 - The MNIST database of handwritten digits.html:/Users/miguel/Library/Application Support/Zotero/Profiles/1fz1fwch.default/zotero/storage/3WAEBHDX/LeCun et al. - 2012 - The MNIST database of handwritten digits.html:text/html}
}

@book{jacod_probability_2004,
	edition = {2},
	series = {Universitext},
	title = {Probability essentials},
	copyright = {Â©2004 Springer-Verlag {BErlin} Heidelberg},
	isbn = {978-3-540-43871-7 (Print), 978-3-642-55682-1 (Online)},
	url = {http://link.springer.com/book/10.1007%2F978-3-642-55682-1},
	abstract = {Almost everyone these days is famihar with the concept of Probability. Each day we are told the probability that it will rain the next day; frequently we discuss the probabilities of winning a lottery or surviving the crash of an airplane. The insurance industry calculates (for example) the probability that a man or woman will live past his or her eightieth birthday, given he or she is 22 years old and applying for life insurance. Probability is used in business too: for example, when deciding to build a waiting area in a restaurant, one wants to calculate the probability of needing space for more than n people each day; a bank wants to calculate the probability a loan will be repaid; a manufacturer wants to calculate the probable demand for his product in the future. In medicine a doctor needs to calculate the probability of success of various alternative remedies; drug companies calculate the probability of harmful side effects of drugs. An example that has recently achieved spectacular success is the use of Probability in Economics, and in particular in Stochastic Finance Theory. Here interest rates and security prices (such as stocks, bonds, currency exchanges) are modelled as varying randomly over time but subject to specific probability laws; one is then able to provide insurance products (for example) to investors by using these models. One could go on with such a list. Probability theory is ubiquitous in modern society and in science.},
	language = {en},
	urldate = {2014-10-19},
	publisher = {Springer Berlin Heidelberg},
	author = {Jacod, Jean and Protter, Philip},
	month = jan,
	year = {2004},
	file = {Jacod and Protter - 2004 - Solutions to exercises in Probability essentials .pdf:/Users/miguel/Library/Application Support/Zotero/Profiles/1fz1fwch.default/zotero/storage/2U8VIM5M/Jacod and Protter - 2004 - Solutions to exercises in Probability essentials .pdf:application/pdf}
}